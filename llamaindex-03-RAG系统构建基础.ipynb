{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex是一个功能强大的框架，让开发者能够轻松构建与自定义数据交互的LLM应用。LlamaIndex数据处理的核心工作流程：加载数据、建立索引、查询响应。在今天的学习中，我们将深入探讨每个步骤的更多细节和高级用法。你将掌握以下使用LlamaIndex构建RAG系统的基础知识：\n",
    "\n",
    "数据加载：从多种源加载文档\n",
    "文本切分：将文档分割成适合检索的小块\n",
    "索引构建：使用向量存储等技术创建检索索引\n",
    "检索实现：基于相似度等方法检索相关内容\n",
    "回答生成：将检索内容与用户问题结合生成回答\n",
    "高级定制：自定义模型、提示词和处理流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载\n",
    "\n",
    "LlamaIndex提供了丰富的数据加载工具，可以从各种来源导入数据。\n",
    "\n",
    "### 1.1 基础文件加载\n",
    "\n",
    "最常用的文档加载器是SimpleDirectoryReader，它可以加载指定目录下的各种格式文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# 基本使用\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"./data/llamaindex_data/\",  # 目标目录\n",
    "    recursive=False,     # 是否递归遍历子目录\n",
    "    required_exts=[\".pdf\", \".txt\"]  # 只读取指定后缀的文件\n",
    ")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个文档都包含文本内容和元数据，可以通过以下方式查看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "{\n",
      "    \"id_\": \"3426b62f-bc1d-4fe0-96ee-2f465a06a768\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"page_label\": \"1\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_path\": \"d:\\\\DataAnalysis\\\\jupyter_notebook_warehouse\\\\bigmodel_learn\\\\ai-learning\\\\data\\\\llamaindex_data\\\\deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2025-03-27\",\n",
      "        \"last_modified_date\": \"2025-03-27\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {},\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text_resource\": {\n",
      "        \"embeddings\": null,\n",
      "        \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\",\n",
      "        \"path\": null,\n",
      "        \"url\": null,\n",
      "        \"mimetype\": null\n",
      "    },\n",
      "    \"image_resource\": null,\n",
      "    \"audio_resource\": null,\n",
      "    \"video_resource\": null,\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"Document\",\n",
      "    \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 查看文档内容\n",
    "print(documents[0].text)\n",
    "\n",
    "# 查看文档完整信息（包括元数据）\n",
    "import json\n",
    "print(json.dumps(documents[0].dict(), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用LlamaParse处理PDF文档\n",
    "\n",
    "对于PDF文件，LlamaIndex提供了更专业的解析器LlamaParse，可以更好地保留文档的结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id dbc57e94-2001-421f-903e-9f131d863c54\n",
      "# DeepSeek-V3 Technical Report\n",
      "\n",
      "# DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "\n",
      "|DeepSeek-V3|DeepSeek-V2.5| |Qwen2.5-72B-Inst| |Llama-3.1-405B-Inst|GPT-4o-0513|Claude-3.5-Sonnet-1022| | |\n",
      "|---|---|---|---|---|---|---|---|---|---|\n",
      "|100|90.2| | | | | | | | |\n",
      "|80|75.9|71.6|73.3|72.6|78.0| | | | |\n",
      "| | | |74.7|80.0|73.8|74.6|78.3| | |\n",
      "|66.2| |65.0| | | | | | | |\n",
      "|60| |59.1| | | | | | | |\n",
      "| |49.0|51.1|49.9| | |51.6|50.8| | |\n",
      "|40|41.3| | |39.2| | |42.0|38.8| |\n",
      "| | | | | | |35.6| | | |\n",
      "|20| | | | | |20.3| | | |\n",
      "| | | | | |16.7|16.0| | | |\n",
      "| | | | | |9.3| | | | |\n",
      "|0|MMLU-Pro|GPQA-Diamond|MATH 500| |AIME 2024|Codeforces|SWE-bench Verified| | |\n",
      "| |(EM)|(Pass@1)|(EM)|(Pass@1)| |(Percentile)|(Resolved)| | |\n",
      "\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n"
     ]
    }
   ],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "# 在Jupyter环境中需要此操作\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 设置解析器\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\"  # 可选\"markdown\"或\"text\"\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./data/llamaindex_data/\", \n",
    "    required_exts=[\".pdf\"], \n",
    "    file_extractor=file_extractor\n",
    ").load_data()\n",
    "\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 加载网页数据\n",
    "\n",
    "LlamaIndex还支持直接从网页加载数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[![](https://qwenlm.github.io/img/logo.png)](/ \"Qwen \\(Alt + H\\)\")\n",
      "\n",
      "  * [Blog](/blog/ \"Blog\")\n",
      "  * [Publication](/publication \"Publication\")\n",
      "  * [About](/about \"About\")\n",
      "  * [Try Qwen Chat ](https://chat.qwen.ai \"Try Qwen Chat\")\n",
      "\n",
      "# Qwen3：思深，行速\n",
      "\n",
      "2025年4月29日 · 4 分钟 · 794 字 · Qwen Team | 语言:\n",
      "\n",
      "  * [English](https://qwenlm.github.io/blog/qwen3/)\n",
      "\n",
      "![Qwen3 Main Image](https://qianwen-res.oss-accelerate-\n",
      "overseas.aliyuncs.com/qwen3-banner.png)\n",
      "\n",
      "[QWEN CHAT](https://chat.qwen.ai) [GitHub](https://github.com/QwenLM/Qwen3)\n",
      "[Hugging\n",
      "Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)\n",
      "[ModelScope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48)\n",
      "[Kaggle](https://www.kaggle.com/models/qwen-lm/qwen-3)\n",
      "[DEMO](https://huggingface.co/spaces/Qwen/Qwen3-Demo)\n",
      "[DISCORD](https://discord.gg/yPEP2vHTu4)\n",
      "\n",
      "## 引言#\n",
      "\n",
      "今天，我们宣布推出 **Qwen3** ，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型 **Qwen3-235B-A22B**\n",
      "在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro\n",
      "等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 **Qwen3-30B-A3B** 的激活参数数量是 QwQ-32B 的\n",
      "10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。\n",
      "\n",
      "![](https://qianwen-res.oss-cn-\n",
      "beijing.aliyuncs.com/Qwen3/qwen3-235a22.jpg)![](https://qianwen-res.oss-cn-\n",
      "beijing.aliyuncs.com/Qwen3/qwen3-30a3.jpg)\n",
      "\n",
      "我们开源了两个 MoE 模型的权重：**Qwen3-235B-A22B** ，一个拥有 2350 多亿总参数和 220\n",
      "多亿激活参数的大模型，以及**Qwen3-30B-A3B** ，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense\n",
      "模型也已开源，包括 **Qwen3-32B** 、**Qwen3-14B** 、**Qwen3-8B** 、**Qwen3-4B**\n",
      "、**Qwen3-1.7B** 和 **Qwen3-0.6B** ，均在 Apache 2.0 许可下开源。\n",
      "\n",
      "Models| Layers| Heads (Q / KV)| Tie Embedding| Context Length  \n",
      "---|---|---|---|---  \n",
      "Qwen3-0.6B| 28| 16 / 8| Yes| 32K  \n",
      "Qwen3-1.7B| 28| 16 / 8| Yes| 32K  \n",
      "Qwen3-4B| 36| 32 / 8| Yes| 32K  \n",
      "Qwen3-8B| 36| 32 / 8| No| 128K  \n",
      "Qwen3-14B| 40| 40 / 8| No| 128K  \n",
      "Qwen3-32B| 64| 64 / 8| No| 128K  \n",
      "Models| Layers| Heads (Q / KV)| # Experts (Total / Activated)| Context Length  \n",
      "---|---|---|---|---  \n",
      "Qwen3-30B-A3B| 48| 32 / 4| 128 / 8| 128K  \n",
      "Qwen3-235B-A22B| 94| 64 / 4| 128 / 8| 128K  \n",
      "  \n",
      "经过后训练的模型，例如 **Qwen3-30B-A3B** ，以及它们的预训练基座模型（如 **Qwen3-30B-A3B-Base** ），现已在\n",
      "**Hugging Face** 、**ModelScope** 和 **Kaggle** 等平台上开放使用。对于部署，我们推荐使用 **SGLang**\n",
      "和 **vLLM** 等框架；而对于本地使用，像 **Ollama** 、**LMStudio** 、**MLX** 、**llama.cpp** 和\n",
      "**KTransformers** 这样的工具也非常值得推荐。这些选项确保用户可以轻松将 Qwen3\n",
      "集成到他们的工作流程中，无论是用于研究、开发还是生产环境。\n",
      "\n",
      "我们相信，Qwen3\n",
      "的发布和开源将极大地推动大型基础模型的研究与开发。我们的目标是为全球的研究人员、开发者和组织赋能，帮助他们利用这些前沿模型构建创新解决方案。\n",
      "\n",
      "欢迎在 Qwen Chat 网页版 ([chat.qwen.ai](https://chat.qwen.ai)) 和手机 APP 中试用 Qwen3！\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "## 核心亮点#\n",
      "\n",
      "  * **多种思考模式**\n",
      "\n",
      "Qwen3 模型支持两种思考模式：\n",
      "\n",
      "  1. 思考模式：在这种模式下，模型会逐步推理，经过深思熟虑后给出最终答案。这种方法非常适合需要深入思考的复杂问题。\n",
      "  2. 非思考模式：在此模式中，模型提供快速、近乎即时的响应，适用于那些对速度要求高于深度的简单问题。\n",
      "\n",
      "这种灵活性使用户能够根据具体任务控制模型进行“思考”的程度。例如，复杂的问题可以通过扩展推理步骤来解决，而简单的问题则可以直接快速作答，无需延迟。至关重要的是，这两种模式的结合大大增强了模型实现稳定且高效的“思考预算”控制能力。如上文所述，Qwen3\n",
      "展现出可扩展且平滑的性能提升，这与分配的计算推理预算直接相关。这样的设计让用户能够更轻松地为不同任务配置特定的预算，在成本效益和推理质量之间实现更优的平衡。\n",
      "\n",
      "![](https://qianwen-res.oss-\n",
      "accelerate.aliyuncs.com/assets/blog/qwen3/thinking_budget.png)\n",
      "\n",
      "  * **多语言**\n",
      "\n",
      "Qwen3 模型支持 **119 种语言和方言** 。这一广泛的多语言能力为国际应用开辟了新的可能性，让全球用户都能受益于这些模型的强大功能。\n",
      "\n",
      "语系| 语种&方言  \n",
      "---|---  \n",
      "印欧语系|\n",
      "英语、法语、葡萄牙语、德语、罗马尼亚语、瑞典语、丹麦语、保加利亚语、俄语、捷克语、希腊语、乌克兰语、西班牙语、荷兰语、斯洛伐克语、克罗地亚语、波兰语、立陶宛语、挪威语（博克马尔语）、挪威尼诺斯克语、波斯语、斯洛文尼亚语、古吉拉特语、拉脱维亚语、意大利语、奥克语、尼泊尔语、马拉地语、白俄罗斯语、塞尔维亚语、卢森堡语、威尼斯语、阿萨姆语、威尔士语、西里西亚语、阿斯图里亚语、恰蒂斯加尔语、阿瓦德语、迈蒂利语、博杰普尔语、信德语、爱尔兰语、法罗语、印地语、旁遮普语、孟加拉语、奥里雅语、塔吉克语、东意第绪语、伦巴第语、利古里亚语、西西里语、弗留利语、撒丁岛语、加利西亚语、加泰罗尼亚语、冰岛语、托斯克语、阿尔巴尼亚语、林堡语、罗马尼亚语、达里语、南非荷兰语、马其顿语僧伽罗语、乌尔都语、马加希语、波斯尼亚语、亚美尼亚语  \n",
      "汉藏语系| 中文（简体中文、繁体中文、粤语）、缅甸语  \n",
      "亚非语系| 阿拉伯语（标准语、内志语、黎凡特语、埃及语、摩洛哥语、美索不达米亚语、塔伊兹-阿德尼语、突尼斯语）、希伯来语、马耳他语  \n",
      "南岛语系| 印度尼西亚语、马来语、他加禄语、宿务语、爪哇语、巽他语、米南加保语、巴厘岛语、班加语、邦阿西楠语、伊洛科语、瓦雷语（菲律宾）  \n",
      "德拉威语| 泰米尔语、泰卢固语、卡纳达语、马拉雅拉姆语  \n",
      "突厥语系| 土耳其语、北阿塞拜疆语、北乌兹别克语、哈萨克语、巴什基尔语、鞑靼语  \n",
      "壮侗语系| 泰语、老挝语  \n",
      "乌拉尔语系| 芬兰语、爱沙尼亚语、匈牙利语  \n",
      "南亚语系| 越南语、高棉语  \n",
      "其他| 日语、韩语、格鲁吉亚语、巴斯克语、海地语、帕皮阿门托语、卡布维尔迪亚努语、托克皮辛语、斯瓦希里语  \n",
      "  \n",
      "  * **增强的 Agent 能力**\n",
      "\n",
      "我们优化了 Qwen3 模型的 Agent 和 代码能力，同时也加强了对 MCP 的支持。下面我们将提供一些示例，展示 Qwen3\n",
      "是如何思考并与环境进行交互的。\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "## 预训练#\n",
      "\n",
      "在预训练方面，Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是在 18 万亿个 token 上进行预训练的，而 Qwen3\n",
      "使用的数据量几乎是其两倍，达到了约 36 万亿个 token，涵盖了 119 种语言和方言。为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF\n",
      "文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，我们利用\n",
      "Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
      "\n",
      "预训练过程分为三个阶段。在第一阶段（S1），模型在超过 30 万亿个 token 上进行了预训练，上下文长度为 4K\n",
      "token。这一阶段为模型提供了基本的语言技能和通用知识。在第二阶段（S2），我们通过增加知识密集型数据（如\n",
      "STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token\n",
      "上进行了预训练。在最后阶段，我们使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
      "\n",
      "![](https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-base.jpg)\n",
      "\n",
      "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense\n",
      "基础模型的整体性能与参数更多的Qwen2.5基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与\n",
      "Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。特别是在 STEM、编码和推理等领域，Qwen3 Dense\n",
      "基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5\n",
      "Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "## 后训练#\n",
      "\n",
      "![](https://qianwen-res.oss-accelerate.aliyuncs.com/assets/blog/qwen3/post-\n",
      "training.png)\n",
      "\n",
      "为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
      "\n",
      "在第一阶段，我们使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM\n",
      "问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
      "\n",
      "在第三阶段，我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。最后，在第四阶段，我们在包括指令遵循、格式遵循和\n",
      "Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "## 开始使用 Qwen3#\n",
      "\n",
      "以下是如何在不同框架中使用 Qwen3 的简单指南。首先，我们提供了一个在 Hugging Face `transformers` 中使用\n",
      "Qwen3-30B-A3B 的标准示例：\n",
      "\n",
      "    \n",
      "    \n",
      "    from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
      "    \n",
      "    model_name = \"Qwen/Qwen3-30B-A3B\"\n",
      "    \n",
      "    # load the tokenizer and the model\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "        model_name,\n",
      "        torch_dtype=\"auto\",\n",
      "        device_map=\"auto\"\n",
      "    )\n",
      "    \n",
      "    # prepare the model input\n",
      "    prompt = \"Give me a short introduction to large language model.\"\n",
      "    messages = [\n",
      "        {\"role\": \"user\", \"content\": prompt}\n",
      "    ]\n",
      "    text = tokenizer.apply_chat_template(\n",
      "        messages,\n",
      "        tokenize=False,\n",
      "        add_generation_prompt=True,\n",
      "        enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
      "    )\n",
      "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
      "    \n",
      "    # conduct text completion\n",
      "    generated_ids = model.generate(\n",
      "        **model_inputs,\n",
      "        max_new_tokens=32768\n",
      "    )\n",
      "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
      "    \n",
      "    # parsing thinking content\n",
      "    try:\n",
      "        # rindex finding 151668 (</think>)\n",
      "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
      "    except ValueError:\n",
      "        index = 0\n",
      "    \n",
      "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
      "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
      "    \n",
      "    print(\"thinking content:\", thinking_content)\n",
      "    print(\"content:\", content)\n",
      "    \n",
      "\n",
      "要禁用思考模式，只需对参数 `enable_thinking` 进行如下修改：\n",
      "\n",
      "    \n",
      "    \n",
      "    text = tokenizer.apply_chat_template(\n",
      "        messages,\n",
      "        tokenize=False,\n",
      "        add_generation_prompt=True,\n",
      "        enable_thinking=False  # True is the default value for enable_thinking.\n",
      "    )\n",
      "    \n",
      "\n",
      "对于部署，您可以使用 `sglang>=0.4.6.post1` 或 `vllm>=0.8.4` 来创建一个与 OpenAI API 兼容的 API\n",
      "endpoint：\n",
      "\n",
      "  * SGLang:\n",
      "    \n",
      "        python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3\n",
      "    \n",
      "\n",
      "  * vLLM:\n",
      "    \n",
      "        vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1\n",
      "    \n",
      "\n",
      "要禁用思考模式，您可以移除参数 `--reasoning-parser`（以及 `--enable-reasoning`）。\n",
      "\n",
      "如果用于本地开发，您可以通过运行简单的命令 `ollama run qwen3:30b-a3b` 来使用 ollama 与模型进行交互。您也可以使用\n",
      "LMStudio 或者 llama.cpp 以及 ktransformers 等代码库进行本地开发。\n",
      "\n",
      "### 高级用法#\n",
      "\n",
      "我们提供了一种软切换机制，允许用户在 `enable_thinking=True` 时动态控制模型的行为。具体来说，您可以在用户提示或系统消息中添加\n",
      "`/think` 和 `/no_think` 来逐轮切换模型的思考模式。在多轮对话中，模型会遵循最近的指令。\n",
      "\n",
      "以下是一个多轮对话的示例：\n",
      "\n",
      "    \n",
      "    \n",
      "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "    \n",
      "    class QwenChatbot:\n",
      "        def __init__(self, model_name=\"Qwen3-30B-A3B/Qwen3-30B-A3B\"):\n",
      "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
      "            self.history = []\n",
      "    \n",
      "        def generate_response(self, user_input):\n",
      "            messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
      "    \n",
      "            text = self.tokenizer.apply_chat_template(\n",
      "                messages,\n",
      "                tokenize=False,\n",
      "                add_generation_prompt=True\n",
      "            )\n",
      "    \n",
      "            inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
      "            response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n",
      "            response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
      "    \n",
      "            # Update history\n",
      "            self.history.append({\"role\": \"user\", \"content\": user_input})\n",
      "            self.history.append({\"role\": \"assistant\", \"content\": response})\n",
      "    \n",
      "            return response\n",
      "    \n",
      "    # Example Usage\n",
      "    if __name__ == \"__main__\":\n",
      "        chatbot = QwenChatbot()\n",
      "    \n",
      "        # First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
      "        user_input_1 = \"How many r's in strawberries?\"\n",
      "        print(f\"User: {user_input_1}\")\n",
      "        response_1 = chatbot.generate_response(user_input_1)\n",
      "        print(f\"Bot: {response_1}\")\n",
      "        print(\"----------------------\")\n",
      "    \n",
      "        # Second input with /no_think\n",
      "        user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n",
      "        print(f\"User: {user_input_2}\")\n",
      "        response_2 = chatbot.generate_response(user_input_2)\n",
      "        print(f\"Bot: {response_2}\") \n",
      "        print(\"----------------------\")\n",
      "    \n",
      "        # Third input with /think\n",
      "        user_input_3 = \"Really? /think\"\n",
      "        print(f\"User: {user_input_3}\")\n",
      "        response_3 = chatbot.generate_response(user_input_3)\n",
      "        print(f\"Bot: {response_3}\")\n",
      "    \n",
      "\n",
      "### Agent 示例#\n",
      "\n",
      "Qwen3 在工具调用能力方面表现出色。我们推荐使用 [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)\n",
      "来充分发挥 Qwen3 的 Agent 能力。Qwen-Agent 内部封装了工具调用模板和工具调用解析器，大大降低了代码复杂性。\n",
      "\n",
      "要定义可用的工具，您可以使用 MCP 配置文件，使用 Qwen-Agent 内置的工具，或者自行集成其他工具。\n",
      "\n",
      "    \n",
      "    \n",
      "    from qwen_agent.agents import Assistant\n",
      "    \n",
      "    # Define LLM\n",
      "    llm_cfg = {\n",
      "        'model': 'Qwen3-30B-A3B',\n",
      "    \n",
      "        # Use the endpoint provided by Alibaba Model Studio:\n",
      "        # 'model_type': 'qwen_dashscope',\n",
      "        # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n",
      "    \n",
      "        # Use a custom endpoint compatible with OpenAI API:\n",
      "        'model_server': 'http://localhost:8000/v1',  # api_base\n",
      "        'api_key': 'EMPTY',\n",
      "    \n",
      "        # Other parameters:\n",
      "        # 'generate_cfg': {\n",
      "        #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n",
      "        #         # Do not add: When the response has been separated by reasoning_content and content.\n",
      "        #         'thought_in_content': True,\n",
      "        #     },\n",
      "    }\n",
      "    \n",
      "    # Define Tools\n",
      "    tools = [\n",
      "        {'mcpServers': {  # You can specify the MCP configuration file\n",
      "                'time': {\n",
      "                    'command': 'uvx',\n",
      "                    'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n",
      "                },\n",
      "                \"fetch\": {\n",
      "                    \"command\": \"uvx\",\n",
      "                    \"args\": [\"mcp-server-fetch\"]\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "      'code_interpreter',  # Built-in tools\n",
      "    ]\n",
      "    \n",
      "    # Define Agent\n",
      "    bot = Assistant(llm=llm_cfg, function_list=tools)\n",
      "    \n",
      "    # Streaming generation\n",
      "    messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\n",
      "    for responses in bot.run(messages=messages):\n",
      "        pass\n",
      "    print(responses)\n",
      "    \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "## Qwen 的朋友们#\n",
      "\n",
      "感谢众多朋友一直以来对 Qwen 的鼎力支持！我们欢迎更多新朋友加入我们的社区，帮助我们变得更好！\n",
      "\n",
      "![](https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-logo.png)\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "## 未来发展#\n",
      "\n",
      "Qwen3\n",
      "代表了我们在通往通用人工智能（AGI）和超级人工智能（ASI）旅程中的一个重要里程碑。通过扩大预训练和强化学习的规模，我们实现了更高层次的智能。我们无缝集成了思考模式与非思考模式，为用户提供了灵活控制思考预算的能力。此外，我们还扩展了对多种语言的支持，帮助全球更多用户。\n",
      "\n",
      "展望未来，我们计划从多个维度提升我们的模型。这包括优化模型架构和训练方法，以实现几个关键目标：扩展数据规模、增加模型大小、延长上下文长度、拓宽模态范围，并利用环境反馈推进强化学习以进行长周期推理。我们认为，我们正从专注于训练模型的时代过渡到以训练\n",
      "Agent 为中心的时代。我们的下一代迭代将为大家的工作和生活带来有意义的进步。\n",
      "\n",
      "(C) 2025 [Qwen](https://qwenlm.github.io/zh/) Powered by\n",
      "[Hugo](https://gohugo.io/)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 安装网页数据解析器\n",
    "# %pip install llama-index-readers-web\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# 加载网页数据\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://qwenlm.github.io/zh/blog/qwen3/\", ]\n",
    ")\n",
    "\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本切分与解析\n",
    "\n",
    "为了便于检索，LlamaIndex将文档切分为更小的文本片段（Node），这个过程称为Chunking。\n",
    "\n",
    "### 2.1 使用TokenTextSplitter切分文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[![](https://qwenlm.github.io/img/logo.png)](/ \"Qwen \\(Alt + H\\)\")\n",
      "\n",
      "  * [Blog](/blog/ \"Blog\")\n",
      "  * [Publication](/publication \"Publication\")\n",
      "  * [About](/about \"About\")\n",
      "  * [Try Qwen Chat ](https://chat.qwen.ai \"Try Qwen Chat\")\n",
      "\n",
      "# Qwen3：思深，行速\n",
      "\n",
      "2025年4月29日 · 4 分钟 · 794 字 · Qwen Team | 语言:\n",
      "\n",
      "  * [English](https://qwenlm.github.io/blog/qwen3/)\n",
      "\n",
      "![Qwen3 Main Image](https://qianwen-res.oss-accelerate-\n",
      "overseas.aliyuncs.com/qwen3-banner.png)\n",
      "\n",
      "[QWEN CHAT](https://chat.qwen.ai) [GitHub](https://github.com/QwenLM/Qwen3)\n",
      "[Hugging\n",
      "Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)\n",
      "[ModelScope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48)\n",
      "[Kaggle](https://www.kaggle.com/models/qwen-lm/qwen-3)\n",
      "[DEMO](https://huggingface.co/spaces/Qwen/Qwen3-Demo)\n",
      "[DISCORD](https://discord.gg/yPEP2vHTu4)\n",
      "\n",
      "## 引言#\n",
      "\n",
      "今天，我们宣布推出 **Qwen3** ，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型 **Qwen3-235B-A22B**\n",
      "在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro\n",
      "等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 **Qwen3-30B-A3B** 的激活参数数量是 QwQ-32B 的\n",
      "10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。\n",
      "\n",
      "![](https://qianwen-res.oss-cn-\n",
      "beijing.aliyuncs.com/Qwen3/qwen3-235a22.jpg)![](https://qianwen-res.oss-cn-\n",
      "beijing.aliyuncs.com/Qwen3/qwen3-30a3.jpg)\n",
      "\n",
      "我们开源了两个 MoE 模型的权重：**Qwen3-235B-A22B** ，一个拥有 2350 多亿总参数和 220\n",
      "多亿激活参数的大模型，以及**Qwen3-30B-A3B** ，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense\n",
      "模型也已开源，包括 **Qwen3-32B** 、**Qwen3-14B** 、**Qwen3-8B** 、**Qwen3-4B**\n",
      "、**Qwen3-1.7B** 和 **Qwen3-0.6B** ，均在 Apache 2.0 许可下开源。\n",
      "\n",
      "Models| Layers| Heads (Q / KV)| Tie Embedding| Context Length  \n",
      "---|---|---|---|---  \n",
      "Qwen3-0.6B| 28| 16 / 8| Yes| 32K  \n",
      "Qwen3-1.7B| 28| 16 / 8| Yes| 32K  \n",
      "Qwen3-4B| 36| 32 / 8| Yes| 32K  \n",
      "Qwen3-8B| 36| 32 / 8| No| 128K  \n",
      "Qwen3-14B| 40| 40 / 8| No| 128K  \n",
      "Qwen3-32B| 64| 64 / 8| No| 128K  \n",
      "Models| Layers| Heads (Q / KV)| # Experts (Total / Activated)| Context Length  \n",
      "---|---|---|---|---  \n",
      "Qwen3-30B-A3B| 48| 32 / 4| 128 /\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# 创建文本分割器\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=1000,    # 每个chunk的最大长度\n",
    "    chunk_overlap=500   # chunk之间重叠长度\n",
    ")\n",
    "\n",
    "# 切分文档\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# 查看节点内容\n",
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex提供了多种文本分割器，满足不同场景需求：\n",
    "\n",
    "SentenceSplitter：在切分时尽量保证句子边界不被切断\n",
    "\n",
    "CodeSplitter：根据AST（抽象语法树）切分代码，保证代码功能片段完整\n",
    "\n",
    "SemanticSplitterNodeParser：根据语义相关性切分文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 使用NodeParsers解析结构化文档\n",
    "\n",
    "对于HTML等结构化文档，可以使用专门的解析器提取有意义的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWEN CHAT\n",
      "GitHub\n",
      "Hugging Face\n",
      "ModelScope\n",
      "Kaggle\n",
      "DEMO\n",
      "DISCORD\n",
      "今天，我们宣布推出\n",
      "Qwen3\n",
      "，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型\n",
      "Qwen3-235B-A22B\n",
      "在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型\n",
      "Qwen3-30B-A3B\n",
      "的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。\n",
      "我们开源了两个 MoE 模型的权重：\n",
      "Qwen3-235B-A22B\n",
      "，一个拥有 2350 多亿总参数和 220 多亿激活参数的大模型，以及\n",
      "Qwen3-30B-A3B\n",
      "，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense 模型也已开源，包括\n",
      "Qwen3-32B\n",
      "、\n",
      "Qwen3-14B\n",
      "、\n",
      "Qwen3-8B\n",
      "、\n",
      "Qwen3-4B\n",
      "、\n",
      "Qwen3-1.7B\n",
      "和\n",
      "Qwen3-0.6B\n",
      "，均在 Apache 2.0 许可下开源。\n",
      "经过后训练的模型，例如\n",
      "Qwen3-30B-A3B\n",
      "，以及它们的预训练基座模型（如\n",
      "Qwen3-30B-A3B-Base\n",
      "），现已在\n",
      "Hugging Face\n",
      "、\n",
      "ModelScope\n",
      "和\n",
      "Kaggle\n",
      "等平台上开放使用。对于部署，我们推荐使用\n",
      "SGLang\n",
      "和\n",
      "vLLM\n",
      "等框架；而对于本地使用，像\n",
      "Ollama\n",
      "、\n",
      "LMStudio\n",
      "、\n",
      "MLX\n",
      "、\n",
      "llama.cpp\n",
      "和\n",
      "KTransformers\n",
      "这样的工具也非常值得推荐。这些选项确保用户可以轻松将 Qwen3 集成到他们的工作流程中，无论是用于研究、开发还是生产环境。\n",
      "我们相信，Qwen3 的发布和开源将极大地推动大型基础模型的研究与开发。我们的目标是为全球的研究人员、开发者和组织赋能，帮助他们利用这些前沿模型构建创新解决方案。\n",
      "欢迎在 Qwen Chat 网页版 (\n",
      "chat.qwen.ai\n",
      ") 和手机 APP 中试用 Qwen3！\n",
      "\n",
      "Qwen3 模型支持两种思考模式：\n",
      "这种灵活性使用户能够根据具体任务控制模型进行“思考”的程度。例如，复杂的问题可以通过扩展推理步骤来解决，而简单的问题则可以直接快速作答，无需延迟。至关重要的是，这两种模式的结合大大增强了模型实现稳定且高效的“思考预算”控制能力。如上文所述，Qwen3 展现出可扩展且平滑的性能提升，这与分配的计算推理预算直接相关。这样的设计让用户能够更轻松地为不同任务配置特定的预算，在成本效益和推理质量之间实现更优的平衡。\n",
      "Qwen3 模型支持\n",
      "119 种语言和方言\n",
      "。这一广泛的多语言能力为国际应用开辟了新的可能性，让全球用户都能受益于这些模型的强大功能。\n",
      "我们优化了 Qwen3 模型的 Agent 和 代码能力，同时也加强了对 MCP 的支持。下面我们将提供一些示例，展示 Qwen3 是如何思考并与环境进行交互的。\n",
      "\n",
      "\n",
      "在预训练方面，Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是在 18 万亿个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 万亿个 token，涵盖了 119 种语言和方言。为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF 文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，我们利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
      "预训练过程分为三个阶段。在第一阶段（S1），模型在超过 30 万亿个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。在第二阶段（S2），我们通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。在最后阶段，我们使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
      "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的Qwen2.5基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
      "\n",
      "为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
      "在第一阶段，我们使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
      "在第三阶段，我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。最后，在第四阶段，我们在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n",
      "\n",
      "以下是如何在不同框架中使用 Qwen3 的简单指南。首先，我们提供了一个在 Hugging Face\n",
      "transformers\n",
      "中使用 Qwen3-30B-A3B 的标准示例：\n",
      "要禁用思考模式，只需对参数\n",
      "enable_thinking\n",
      "进行如下修改：\n",
      "对于部署，您可以使用\n",
      "sglang>=0.4.6.post1\n",
      "或\n",
      "vllm>=0.8.4\n",
      "来创建一个与 OpenAI API 兼容的 API endpoint：\n",
      "SGLang:\n",
      "vLLM:\n",
      "要禁用思考模式，您可以移除参数\n",
      "--reasoning-parser\n",
      "（以及\n",
      "--enable-reasoning\n",
      "）。\n",
      "如果用于本地开发，您可以通过运行简单的命令\n",
      "ollama run qwen3:30b-a3b\n",
      "来使用 ollama 与模型进行交互。您也可以使用 LMStudio 或者 llama.cpp 以及 ktransformers 等代码库进行本地开发。\n",
      "我们提供了一种软切换机制，允许用户在\n",
      "enable_thinking=True\n",
      "时动态控制模型的行为。具体来说，您可以在用户提示或系统消息中添加\n",
      "/think\n",
      "和\n",
      "/no_think\n",
      "来逐轮切换模型的思考模式。在多轮对话中，模型会遵循最近的指令。\n",
      "以下是一个多轮对话的示例：\n",
      "Qwen3 在工具调用能力方面表现出色。我们推荐使用\n",
      "Qwen-Agent\n",
      "来充分发挥 Qwen3 的 Agent 能力。Qwen-Agent 内部封装了工具调用模板和工具调用解析器，大大降低了代码复杂性。\n",
      "要定义可用的工具，您可以使用 MCP 配置文件，使用 Qwen-Agent 内置的工具，或者自行集成其他工具。\n",
      "\n",
      "感谢众多朋友一直以来对 Qwen 的鼎力支持！我们欢迎更多新朋友加入我们的社区，帮助我们变得更好！\n",
      "\n",
      "Qwen3 代表了我们在通往通用人工智能（AGI）和超级人工智能（ASI）旅程中的一个重要里程碑。通过扩大预训练和强化学习的规模，我们实现了更高层次的智能。我们无缝集成了思考模式与非思考模式，为用户提供了灵活控制思考预算的能力。此外，我们还扩展了对多种语言的支持，帮助全球更多用户。\n",
      "展望未来，我们计划从多个维度提升我们的模型。这包括优化模型架构和训练方法，以实现几个关键目标：扩展数据规模、增加模型大小、延长上下文长度、拓宽模态范围，并利用环境反馈推进强化学习以进行长周期推理。我们认为，我们正从专注于训练模型的时代过渡到以训练 Agent 为中心的时代。我们的下一代迭代将为大家的工作和生活带来有意义的进步。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# 加载HTML文档\n",
    "documents = SimpleWebPageReader(html_to_text=False).load_data([\"https://qwenlm.github.io/zh/blog/qwen3/\"])\n",
    "\n",
    "# 解析HTML标签\n",
    "# 默认解析 [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n",
    "parser = HTMLNodeParser(tags=[\"p\"])  # 可以自定义解析哪些标签\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 索引与检索\n",
    "索引是为实现快速检索而设计的特定数据结构。LlamaIndex提供多种索引类型，最常用的是向量索引。\n",
    "\n",
    "### 3.1 向量检索\n",
    "VectorStoreIndex是最基础的索引类型，将文本转换为向量并存储在内存中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相关度分数: 0.8638171847866085\n",
      "QWEN CHAT\n",
      "GitHub\n",
      "Hugging Face\n",
      "ModelScope\n",
      "Kaggle\n",
      "DEMO\n",
      "DISCORD\n",
      "今天，我们宣布推出\n",
      "Qwen3\n",
      "，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型\n",
      "Qwen3-235B-A22B\n",
      "在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型\n",
      "Qwen3-30B-A3B\n",
      "的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。\n",
      "我们开源了两个 MoE 模型的权重：\n",
      "Qwen3-235B-A22B\n",
      "，一个拥有 2350 多亿总参数和 220 多亿激活参数的大模型，以及\n",
      "Qwen3-30B-A3B\n",
      "，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense 模型也已开源，包括\n",
      "Qwen3-32B\n",
      "、\n",
      "Qwen3-14B\n",
      "、\n",
      "Qwen3-8B\n",
      "、\n",
      "Qwen3-4B\n",
      "、\n",
      "Qwen3-1.7B\n",
      "和\n",
      "Qwen3-0.6B\n",
      "，均在 Apache 2.0 许可下开源。\n",
      "经过后训练的模型，例如\n",
      "Qwen3-30B-A3B\n",
      "，以及它们的预训练基座模型（如\n",
      "Qwen3-30B-A3B-Base\n",
      "），现已在\n",
      "Hugging Face\n",
      "、\n",
      "ModelScope\n",
      "和\n",
      "Kaggle\n",
      "等平台上开放使用。对于部署，我们推荐使用\n",
      "SGLang\n",
      "和\n",
      "vLLM\n",
      "等框架；而对于本地使用，像\n",
      "Ollama\n",
      "、\n",
      "LMStudio\n",
      "、\n",
      "MLX\n",
      "、\n",
      "llama.cpp\n",
      "和\n",
      "KTransformers\n",
      "这样的工具也非常值得推荐。这些选项确保用户可以轻松将 Qwen3 集成到他们的工作流程中，无论是用于研究、开发还是生产环境。\n",
      "我们相信，Qwen3 的发布和开源将极大地推动大型基础模型的研究与开发。我们的目标是为全球的研究人员、开发者和组织赋能，帮助他们利用这些前沿模型构建创新解决方案。\n",
      "欢迎在 Qwen Chat 网页版 (\n",
      "chat.qwen.ai\n",
      ") 和手机 APP 中试用 Qwen3！\n",
      "\n",
      "Qwen3 模型支持两种思考模式：\n",
      "这种灵活性使用户能够根据具体任务控制模型进行“思考”的程度。例如，复杂的问题可以通过扩展推理步骤来解决，而简单的问题则可以直接快速作答，无需延迟。至关重要的是，这两种模式的结合大大增强了模型实现稳定且高效的“思考预算”控制能力。如上文所述，Qwen3 展现出可扩展且平滑的性能提升，这与分配的计算推理预算直接相关。这样的设计让用户能够更轻松地为不同任务配置特定的预算，在成本效益和推理质量之间实现更优的平衡。\n",
      "Qwen3 模型支持\n",
      "119 种语言和方言\n",
      "。这一广泛的多语言能力为国际应用开辟了新的可能性，让全球用户都能受益于这些模型的强大功能。\n",
      "我们优化了 Qwen3 模型的 Agent 和 代码能力，同时也加强了对 MCP 的支持。下面我们将提供一些示例，展示 Qwen3 是如何思考并与环境进行交互的。\n",
      "\n",
      "\n",
      "在预训练方面，Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是在 18 万亿个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 万亿个 token，涵盖了 119 种语言和方言。为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF 文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，我们利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
      "预训练过程分为三个阶段。在第一阶段（S1），模型在超过 30 万亿个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。在第二阶段（S2），我们通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。在最后阶段，我们使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
      "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的Qwen2.5基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
      "\n",
      "为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
      "在第一阶段，我们使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
      "在第三阶段，我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。最后，在第四阶段，我们在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n",
      "\n",
      "以下是如何在不同框架中使用 Qwen3 的简单指南。首先，我们提供了一个在 Hugging Face\n",
      "transformers\n",
      "中使用 Qwen3-30B-A3B 的标准示例：\n",
      "要禁用思考模式，只需对参数\n",
      "enable_thinking\n",
      "进行如下修改：\n",
      "对于部署，您可以使用\n",
      "sglang>=0.4.6.post1\n",
      "或\n",
      "vllm>=0.8.4\n",
      "来创建一个与 OpenAI API 兼容的 API endpoint：\n",
      "SGLang:\n",
      "vLLM:\n",
      "要禁用思考模式，您可以移除参数\n",
      "--reasoning-parser\n",
      "（以及\n",
      "--enable-reasoning\n",
      "）。\n",
      "如果用于本地开发，您可以通过运行简单的命令\n",
      "ollama run qwen3:30b-a3b\n",
      "来使用 ollama 与模型进行交互。您也可以使用 LMStudio 或者 llama.cpp 以及 ktransformers 等代码库进行本地开发。\n",
      "我们提供了一种软切换机制，允许用户在\n",
      "enable_thinking=True\n",
      "时动态控制模型的行为。具体来说，您可以在用户提示或系统消息中添加\n",
      "/think\n",
      "和\n",
      "/no_think\n",
      "来逐轮切换模型的思考模式。在多轮对话中，模型会遵循最近的指令。\n",
      "以下是一个多轮对话的示例：\n",
      "Qwen3 在工具调用能力方面表现出色。我们推荐使用\n",
      "Qwen-Agent\n",
      "来充分发挥 Qwen3 的 Agent 能力。Qwen-Agent 内部封装了工具调用模板和工具调用解析器，大大降低了代码复杂性。\n",
      "要定义可用的工具，您可以使用 MCP 配置文件，使用 Qwen-Agent 内置的工具，或者自行集成其他工具。\n",
      "\n",
      "感谢众多朋友一直以来对 Qwen 的鼎力支持！我们欢迎更多新朋友加入我们的社区，帮助我们变得更好！\n",
      "\n",
      "Qwen3 代表了我们在通往通用人工智能（AGI）和超级人工智能（ASI）旅程中的一个重要里程碑。通过扩大预训练和强化学习的规模，我们实现了更高层次的智能。我们无缝集成了思考模式与非思考模式，为用户提供了灵活控制思考预算的能力。此外，我们还扩展了对多种语言的支持，帮助全球更多用户。\n",
      "展望未来，我们计划从多个维度提升我们的模型。这包括优化模型架构和训练方法，以实现几个关键目标：扩展数据规模、增加模型大小、延长上下文长度、拓宽模态范围，并利用环境反馈推进强化学习以进行长周期推理。我们认为，我们正从专注于训练模型的时代过渡到以训练 Agent 为中心的时代。我们的下一代迭代将为大家的工作和生活带来有意义的进步。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 构建索引\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# 创建检索器\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3  # 返回最相似的3个结果\n",
    ")\n",
    "\n",
    "# 检索\n",
    "results = retriever.retrieve(\"qwen3是什么?\")\n",
    "\n",
    "# 查看检索结果\n",
    "for result in results:\n",
    "    print(f\"相关度分数: {result.score}\")\n",
    "    print(result.text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，LlamaIndex使用OpenAI的Embedding模型，但你可以轻松替换为其他模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用外部向量数据库\n",
    "\n",
    "在实际应用中，通常需要使用专门的向量数据库存储向量以提高性能和持久化数据。以Qdrant为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWEN CHAT\n",
      "GitHub\n",
      "Hugging Face\n",
      "ModelScope\n",
      "Kaggle\n",
      "DEMO\n",
      "DISCORD\n",
      "今天，我们宣布推出\n",
      "Qwen3\n",
      "，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型\n",
      "Qwen3-235B-A22B\n",
      "在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型\n",
      "Qwen3-30B-A3B\n",
      "的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。\n",
      "我们开源了两个 MoE 模型的权重：\n",
      "Qwen3-235B-A22B\n",
      "，一个拥有 2350 多亿总参数和 220 多亿激活参数的大模型，以及\n",
      "Qwen3-30B-A3B\n",
      "，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense 模型也已开源，包括\n",
      "Qwen3-32B\n",
      "、\n",
      "Qwen3-14B\n",
      "、\n",
      "Qwen3-8B\n",
      "、\n",
      "Qwen3-4B\n",
      "、\n",
      "Qwen3-1.7B\n",
      "和\n",
      "Qwen3-0.6B\n",
      "，均在 Apache 2.0 许可下开源。\n",
      "经过后训练的模型，例如\n",
      "Qwen3-30B-A3B\n",
      "，以及它们的预训练基座模型（如\n",
      "Qwen3-30B-A3B-Base\n",
      "），现已在\n",
      "Hugging Face\n",
      "、\n",
      "ModelScope\n",
      "和\n",
      "Kaggle\n",
      "等平台上开放使用。对于部署，我们推荐使用\n",
      "SGLang\n",
      "和\n",
      "vLLM\n",
      "等框架；而对于本地使用，像\n",
      "Ollama\n",
      "、\n",
      "LMStudio\n",
      "、\n",
      "MLX\n",
      "、\n",
      "llama.cpp\n",
      "和\n",
      "KTransformers\n",
      "这样的工具也非常值得推荐。这些选项确保用户可以轻松将 Qwen3 集成到他们的工作流程中，无论是用于研究、开发还是生产环境。\n",
      "我们相信，Qwen3 的发布和开源将极大地推动大型基础模型的研究与开发。我们的目标是为全球的研究人员、开发者和组织赋能，帮助他们利用这些前沿模型构建创新解决方案。\n",
      "欢迎在 Qwen Chat 网页版 (\n",
      "chat.qwen.ai\n",
      ") 和手机 APP 中试用 Qwen3！\n",
      "\n",
      "Qwen3 模型支持两种思考模式：\n",
      "这种灵活性使用户能够根据具体任务控制模型进行“思考”的程度。例如，复杂的问题可以通过扩展推理步骤来解决，而简单的问题则可以直接快速作答，无需延迟。至关重要的是，这两种模式的结合大大增强了模型实现稳定且高效的“思考预算”控制能力。如上文所述，Qwen3 展现出可扩展且平滑的性能提升，这与分配的计算推理预算直接相关。这样的设计让用户能够更轻松地为不同任务配置特定的预算，在成本效益和推理质量之间实现更优的平衡。\n",
      "Qwen3 模型支持\n",
      "119 种语言和方言\n",
      "。这一广泛的多语言能力为国际应用开辟了新的可能性，让全球用户都能受益于这些模型的强大功能。\n",
      "我们优化了 Qwen3 模型的 Agent 和 代码能力，同时也加强了对 MCP 的支持。下面我们将提供一些示例，展示 Qwen3 是如何思考并与环境进行交互的。\n",
      "\n",
      "\n",
      "在预训练方面，Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是在 18 万亿个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 万亿个 token，涵盖了 119 种语言和方言。为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF 文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，我们利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
      "预训练过程分为三个阶段。在第一阶段（S1），模型在超过 30 万亿个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。在第二阶段（S2），我们通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。在最后阶段，我们使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
      "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的Qwen2.5基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
      "\n",
      "为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
      "在第一阶段，我们使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
      "在第三阶段，我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。最后，在第四阶段，我们在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n",
      "\n",
      "以下是如何在不同框架中使用 Qwen3 的简单指南。首先，我们提供了一个在 Hugging Face\n",
      "transformers\n",
      "中使用 Qwen3-30B-A3B 的标准示例：\n",
      "要禁用思考模式，只需对参数\n",
      "enable_thinking\n",
      "进行如下修改：\n",
      "对于部署，您可以使用\n",
      "sglang>=0.4.6.post1\n",
      "或\n",
      "vllm>=0.8.4\n",
      "来创建一个与 OpenAI API 兼容的 API endpoint：\n",
      "SGLang:\n",
      "vLLM:\n",
      "要禁用思考模式，您可以移除参数\n",
      "--reasoning-parser\n",
      "（以及\n",
      "--enable-reasoning\n",
      "）。\n",
      "如果用于本地开发，您可以通过运行简单的命令\n",
      "ollama run qwen3:30b-a3b\n",
      "来使用 ollama 与模型进行交互。您也可以使用 LMStudio 或者 llama.cpp 以及 ktransformers 等代码库进行本地开发。\n",
      "我们提供了一种软切换机制，允许用户在\n",
      "enable_thinking=True\n",
      "时动态控制模型的行为。具体来说，您可以在用户提示或系统消息中添加\n",
      "/think\n",
      "和\n",
      "/no_think\n",
      "来逐轮切换模型的思考模式。在多轮对话中，模型会遵循最近的指令。\n",
      "以下是一个多轮对话的示例：\n",
      "Qwen3 在工具调用能力方面表现出色。我们推荐使用\n",
      "Qwen-Agent\n",
      "来充分发挥 Qwen3 的 Agent 能力。Qwen-Agent 内部封装了工具调用模板和工具调用解析器，大大降低了代码复杂性。\n",
      "要定义可用的工具，您可以使用 MCP 配置文件，使用 Qwen-Agent 内置的工具，或者自行集成其他工具。\n",
      "\n",
      "感谢众多朋友一直以来对 Qwen 的鼎力支持！我们欢迎更多新朋友加入我们的社区，帮助我们变得更好！\n",
      "\n",
      "Qwen3 代表了我们在通往通用人工智能（AGI）和超级人工智能（ASI）旅程中的一个重要里程碑。通过扩大预训练和强化学习的规模，我们实现了更高层次的智能。我们无缝集成了思考模式与非思考模式，为用户提供了灵活控制思考预算的能力。此外，我们还扩展了对多种语言的支持，帮助全球更多用户。\n",
      "展望未来，我们计划从多个维度提升我们的模型。这包括优化模型架构和训练方法，以实现几个关键目标：扩展数据规模、增加模型大小、延长上下文长度、拓宽模态范围，并利用环境反馈推进强化学习以进行长周期推理。我们认为，我们正从专注于训练模型的时代过渡到以训练 Agent 为中心的时代。我们的下一代迭代将为大家的工作和生活带来有意义的进步。\n"
     ]
    }
   ],
   "source": [
    "# 安装Qdrant扩展\n",
    "# %pip install llama-index-vector-stores-qdrant\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# 创建Qdrant客户端\n",
    "client = QdrantClient(location=\":memory:\")  # 生产环境可使用真实服务器地址\n",
    "collection_name = \"my_collection\"\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 创建向量存储和索引\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "# 检索\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "results = retriever.retrieve(\"qwen3是什么?\")\n",
    "\n",
    "print(results[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 检索后处理\n",
    "\n",
    "LlamaIndex提供了检索后处理模块，可以进一步优化检索结果。例如，使用语言模型对检索结果重新排序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] QWEN CHAT\n",
      "GitHub\n",
      "Hugging Face\n",
      "ModelScope\n",
      "Kaggle\n",
      "DEMO\n",
      "DISCORD\n",
      "今天，我们宣布推出\n",
      "Qwen3\n",
      "，这是 Qwen 系列大型语言模型的最新成员。我们的旗舰模型\n",
      "Qwen3-235B-A22B\n",
      "在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型\n",
      "Qwen3-30B-A3B\n",
      "的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# 创建检索器并获取结果\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "nodes = retriever.retrieve(\"qwen3是什么?\")\n",
    "\n",
    "# 使用LLM重排序，保留最相关的前2个结果\n",
    "postprocessor = LLMRerank(top_n=2)\n",
    "reranked_nodes = postprocessor.postprocess_nodes(\n",
    "    nodes, \n",
    "    query_str=\"qwen3是什么?\"\n",
    ")\n",
    "\n",
    "# 查看重排序后的结果\n",
    "for i, node in enumerate(reranked_nodes):\n",
    "    print(f\"[{i}] {node.text[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 生成回复\n",
    "\n",
    "LlamaIndex提供两种主要的响应接口：QueryEngine（单轮问答）和ChatEngine（多轮对话）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 单轮问答（QueryEngine）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3 是一系列大型语言模型中的最新成员，包括 Qwen3-235B-A22B 和 Qwen3-30B-A3B 等模型。这些模型在代码、数学、通用能力等基准测试中表现出极具竞争力的结果，并已经在多个平台上开放使用。其支持两种思考模式，拥有广泛的多语言能力，以及经过优化的 Agent 和代码能力，同时加强了对 MCP 的支持。\n",
      "Qwen3 是一系列大型语言模型中的最新成员，包括 Qwen3-235B-A22B 和 Qwen3-30B-A3B 等模型。这些模型在代码、数学、通用能力等基准测试中表现出极具竞争力的结果，并已在 Hugging Face、ModelScope 和 Kaggle 等平台上开放使用。Qwen3 模型支持两种思考模式，具有灵活性，能够根据具体任务控制模型进行“思考”的程度。此外，Qwen3 模型支持 119 种语言和方言，为国际应用开辟了新的可能性。"
     ]
    }
   ],
   "source": [
    "# 创建问答引擎\n",
    "qa_engine = index.as_query_engine()\n",
    "\n",
    "# 提问\n",
    "response = qa_engine.query(\"qwen3是什么?\")\n",
    "print(response)\n",
    "\n",
    "# 流式输出\n",
    "streaming_qa_engine = index.as_query_engine(streaming=True)\n",
    "response = streaming_qa_engine.query(\"qwen3是什么?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 多轮对话（ChatEngine）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的第一个问题是关于 Qwen3 及其主要特点和功能的。\n",
      "Qwen3 是通往通用人工智能（AGI）和超级人工智能（ASI）的重要里程碑。它通过扩大预训练和强化学习的规模实现了更高水平的智能。Qwen3 无缝集成了思考和非思考模式，使用户能够灵活控制他们的思考预算。此外，它支持多种语言，使全球用户受益。展望未来，计划从多个维度增强模型，包括优化模型架构和训练方法，以实现扩展数据规模、增加模型大小、延长上下文长度、拓展模态以及利用环境反馈推进长期推理的关键目标。这一转变被视为从专注于训练模型的时代转向专注于训练代理的时代，预计下一代迭代将为工作和生活带来有意义的进展。\n",
      "请提供更具体的信息，这样我才能帮助您更好。您想了解什么样的信息？"
     ]
    }
   ],
   "source": [
    "# 创建聊天引擎\n",
    "chat_engine = index.as_chat_engine()\n",
    "\n",
    "# 第一个问题\n",
    "response1 = chat_engine.chat(\"我的第一个问题是什么?\")\n",
    "print(response1)\n",
    "\n",
    "# 跟进问题（保持上下文）\n",
    "response2 = chat_engine.chat(\"还有其他相关信息吗?\")\n",
    "print(response2)\n",
    "\n",
    "# 流式输出\n",
    "streaming_chat_engine = index.as_chat_engine()\n",
    "streaming_response = streaming_chat_engine.stream_chat(\"我想了解更多信息\")\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定制模型与Prompt\n",
    "\n",
    "LlamaIndex提供了灵活的底层接口，允许开发者定制语言模型和提示词模板。\n",
    "\n",
    "### 5.1 Prompt模板\n",
    "\n",
    "使用PromptTemplate创建简单的提示词模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写一个关于人工智能的500字短文\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# 简单模板\n",
    "prompt = PromptTemplate(\"写一个关于{topic}的{length}字短文\")\n",
    "formatted_prompt = prompt.format(topic=\"人工智能\", length=\"500\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用ChatPromptTemplate创建多轮对话模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: 你是一个专业的人工智能助手，擅长解答技术问题。\n",
      "user: 请根据以下上下文回答我的问题：\n",
      "LlamaIndex是一个数据框架，连接LLM与外部数据。\n",
      "\n",
      "问题：LlamaIndex有什么用途?\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "# 创建对话模板\n",
    "chat_messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"你是一个专业的{role}，擅长解答{domain}问题。\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "            \"请根据以下上下文回答我的问题：\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"问题：{question}\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_template = ChatPromptTemplate(chat_messages)\n",
    "formatted_chat = chat_template.format(\n",
    "    role=\"人工智能助手\",\n",
    "    domain=\"技术\",\n",
    "    context=\"LlamaIndex是一个数据框架，连接LLM与外部数据。\",\n",
    "    question=\"LlamaIndex有什么用途?\"\n",
    ")\n",
    "\n",
    "print(formatted_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 使用不同的语言模型\n",
    "\n",
    "LlamaIndex支持多种语言模型，以下是一些例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-dashscope\n",
      "  Downloading llama_index_llms_dashscope-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting dashscope==1.22.2 (from llama-index-llms-dashscope)\n",
      "  Downloading dashscope-1.22.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-llms-dashscope) (0.12.30)\n",
      "Requirement already satisfied: aiohttp in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope==1.22.2->llama-index-llms-dashscope) (3.11.12)\n",
      "Requirement already satisfied: requests in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope==1.22.2->llama-index-llms-dashscope) (2.32.3)\n",
      "Requirement already satisfied: websocket-client in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope==1.22.2->llama-index-llms-dashscope) (1.8.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2.0.38)\n",
      "Requirement already satisfied: banks<3.0.0,>=2.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2.0.0)\n",
      "Requirement already satisfied: dataclasses-json in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2024.2.0)\n",
      "Requirement already satisfied: httpx in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (3.9.1)\n",
      "Requirement already satisfied: numpy in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (10.2.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2.10.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.9.0)\n",
      "Requirement already satisfied: wrapt in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope==1.22.2->llama-index-llms-dashscope) (1.18.3)\n",
      "Requirement already satisfied: griffe in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.6.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (3.1.3)\n",
      "Requirement already satisfied: click in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2024.5.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope==1.22.2->llama-index-llms-dashscope) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope==1.22.2->llama-index-llms-dashscope) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope==1.22.2->llama-index-llms-dashscope) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope==1.22.2->llama-index-llms-dashscope) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (3.1.1)\n",
      "Requirement already satisfied: colorama in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (3.26.1)\n",
      "Requirement already satisfied: anyio in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (24.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-dashscope) (2.1.5)\n",
      "Downloading llama_index_llms_dashscope-0.3.2-py3-none-any.whl (8.1 kB)\n",
      "Downloading dashscope-1.22.2-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.3 MB 187.9 kB/s eta 0:00:07\n",
      "    --------------------------------------- 0.0/1.3 MB 187.9 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.0/1.3 MB 151.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.0/1.3 MB 151.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.0/1.3 MB 151.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.0/1.3 MB 151.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.0/1.3 MB 151.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.0/1.3 MB 151.3 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.3 MB 143.4 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.3 MB 145.8 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 275.8 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 275.8 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.2/1.3 MB 283.2 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.2/1.3 MB 283.2 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.3 MB 295.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.3 MB 295.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.3 MB 295.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.3 MB 295.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.3 MB 295.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.3 MB 295.5 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.3/1.3 MB 298.9 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.3/1.3 MB 298.9 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 305.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.3 MB 361.2 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.6/1.3 MB 445.2 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.6/1.3 MB 453.3 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.6/1.3 MB 454.9 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.7/1.3 MB 471.5 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.7/1.3 MB 505.9 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 0.9/1.3 MB 572.7 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.3 MB 602.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.3 MB 615.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.3 MB 615.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.3 MB 615.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.3 MB 615.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.3 MB 615.6 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 542.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.1/1.3 MB 610.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 660.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 657.1 kB/s eta 0:00:00\n",
      "Installing collected packages: dashscope, llama-index-llms-dashscope\n",
      "Successfully installed dashscope-1.22.2 llama-index-llms-dashscope-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex 是一个强大的工具，可以用于多种场景，特别是在数据索引和搜索方面。以下是五个使用 LlamaIndex 的场景：\n",
      "\n",
      "1. **文档管理系统**：在企业或组织中，LlamaIndex 可以用于创建一个高效的文档管理系统。通过对大量文档进行索引，用户可以快速搜索和检索所需的信息，提高工作效率。\n",
      "\n",
      "2. **电子商务网站搜索**：在电子商务平台上，LlamaIndex 可以帮助实现快速、准确的产品搜索功能。通过对产品描述、用户评论和其他相关数据进行索引，用户可以更容易地找到他们感兴趣的产品。\n",
      "\n",
      "3. **学术研究数据库**：研究人员可以使用 LlamaIndex 来索引和搜索学术论文、研究报告和其他相关文献。这有助于快速查找相关研究成果，支持文献综述和研究分析。\n",
      "\n",
      "4. **客户支持系统**：在客户支持中心，LlamaIndex 可以用于索引常见问题解答（FAQ）、支持文档和历史客户交互记录，从而帮助客服人员快速找到解决方案，提高客户满意度。\n",
      "\n",
      "5. **内容推荐系统**：LlamaIndex 可以用于内容推荐引擎，通过对用户行为数据和内容特征进行索引，帮助提供个性化的内容推荐，提高用户参与度和留存率。\n",
      "\n",
      "这些场景展示了 LlamaIndex 在不同领域的广泛应用潜力，帮助组织和个人更高效地管理和利用信息。\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++\n",
      "检索增强生成（Retrieval-Augmented Generation, RAG）是一种结合信息检索与文本生成的技术，旨在提升生成式模型（如大语言模型）的准确性和事实性。其核心思想是通过实时检索外部知识库，为生成过程提供相关参考信息，从而减少模型因依赖内部参数化知识而产生的错误或幻觉（Hallucination）。\n",
      "\n",
      "### **核心流程**\n",
      "1. **检索（Retrieval）**  \n",
      "   当用户输入查询（Query）时，系统从外部数据库（如维基百科、专业文档等）检索与问题最相关的文档或段落。通常使用向量检索技术（如稠密检索Dense Retrieval），将查询和文档编码为向量，通过相似度匹配找出相关内容。\n",
      "\n",
      "2. **增强（Augmentation）**  \n",
      "   检索到的文档片段与用户查询一起拼接，作为生成模型的输入上下文。例如：  \n",
      "   ```\n",
      "   输入 = [用户问题] + [检索到的相关文档1] + [相关文档2] + ...\n",
      "   ```\n",
      "\n",
      "3. **生成（Generation）**  \n",
      "   生成模型（如GPT、LLaMA等）基于检索到的信息生成回答，确保输出内容与事实一致，而非仅依赖模型训练时的记忆。\n",
      "\n",
      "### **优势**\n",
      "- **事实性更强**：动态引入最新或领域特定知识，避免模型过时或虚构信息。\n",
      "- **可解释性**：可通过检索来源追溯答案依据。\n",
      "- **灵活性**：无需重新训练模型，仅更新检索库即可扩展知识。\n",
      "\n",
      "### **应用场景**\n",
      "- 问答系统（如客服、医疗咨询）\n",
      "- 需要引用外部资料的文本生成（如报告撰写）\n",
      "- 处理专业领域或实时信息（如新闻摘要）\n",
      "\n",
      "### **示例**\n",
      "- **用户问**：“量子计算的最新进展是什么？”  \n",
      "- **RAG步骤**：  \n",
      "  1. 检索2023年arXiv上关于量子计算的论文摘要。  \n",
      "  2. 生成模型结合这些摘要生成总结性回答。\n",
      "\n",
      "### **对比传统生成模型**\n",
      "- **纯生成模型**：依赖训练数据，可能输出过时或错误信息。  \n",
      "- **RAG**：通过检索动态补充知识，输出更可靠。\n",
      "\n",
      "### **挑战**\n",
      "- 检索效率与精度需平衡。\n",
      "- 噪声文档可能干扰生成结果。\n",
      "\n",
      "RAG通过融合检索与生成的优势，成为解决大模型“知识固化”问题的有效方案之一。\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++\n",
      "检索增强生成（Retrieval-Augmented Generation, RAG）是一种结合了信息检索和文本生成技术的方法，旨在提高生成模型在特定任务上的性能。这种方法主要通过以下步骤实现：\n",
      "\n",
      "1. **信息检索**：首先，使用一个检索系统（如基于BM25、TF-IDF等传统方法或更先进的神经网络检索模型）从大量文档中找到与给定查询最相关的几个文档片段。\n",
      "\n",
      "2. **融合检索结果**：将这些检索到的文档片段作为额外输入提供给生成模型。这一步骤可以通过多种方式完成，比如直接将文本拼接到原始输入之后，或者采用更复杂的方式，例如编码检索结果并与原始输入共同处理。\n",
      "\n",
      "3. **生成输出**：最后，利用改进后的输入数据训练或微调预训练语言模型，并让其根据新的上下文生成回答或其他类型的输出。\n",
      "\n",
      "RAG的主要优势在于它能够有效利用外部知识库来增强模型的理解能力和生成质量，特别是在那些需要依赖具体事实或专业领域知识的任务上表现尤为突出。此外，由于不需要对整个知识库进行端到端训练，因此也相对节省计算资源。这种方法已经被成功应用于问答系统、对话系统等多个自然语言处理场景中。\n"
     ]
    }
   ],
   "source": [
    "# OpenAI模型\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm_openai = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 使用语言模型生成文本\n",
    "response = llm_openai.complete(\"列出5个使用LlamaIndex的场景\")\n",
    "print(response.text)\n",
    "\n",
    "# 使用本地模型或其他API提供商\n",
    "# 以DeepSeek为例\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "\n",
    "# DeepSeek模型\n",
    "llm_deepseek = DeepSeek(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 阿里通义千问模型\n",
    "llm_qwen = DashScope(\n",
    "    api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n",
    "    model=\"qwen-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "response = llm_deepseek.complete(\"简单介绍下什么是检索增强生成?\")\n",
    "print(response.text)\n",
    "\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "response = llm_qwen.complete(\"简单介绍下什么是检索增强生成?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 构建完整的RAG系统\n",
    "\n",
    "最后，让我们综合前面所学的知识，构建一个完整的RAG系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 4 个文档\n",
      "生成了 4 个文本节点\n",
      "\n",
      "正在生成回答...\n",
      "\n",
      "DeepSeek-V3 excels in a variety of tasks, including educational benchmarks, factuality benchmarks, math-related tasks, and coding-related tasks. It performs exceptionally well on educational benchmarks like MMLU, MMLU-Pro, and GPQA, and demonstrates strong capabilities in factual knowledge, particularly in Chinese. Additionally, it achieves state-of-the-art performance on math-related benchmarks and is a top performer in coding competitions, making it highly competitive across diverse technical benchmarks."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 全局设置\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 200\n",
    "\n",
    "# 1. 加载文档\n",
    "documents = SimpleDirectoryReader(\"./data/llamaindex_data/\").load_data()\n",
    "print(f\"加载了 {len(documents)} 个文档\")\n",
    "\n",
    "# 2. 切分文档\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=Settings.chunk_size,\n",
    "    chunk_overlap=Settings.chunk_overlap\n",
    ")\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "print(f\"生成了 {len(nodes)} 个文本节点\")\n",
    "\n",
    "# 3. 创建索引\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# 4. 保存索引（可选）\n",
    "index.storage_context.persist(\"./storage\")\n",
    "\n",
    "# 5. 创建查询引擎\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# 6. 创建对话引擎\n",
    "chat_engine = index.as_chat_engine(\n",
    "    similarity_top_k=3,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# 7. 与系统交互\n",
    "while True:\n",
    "    query = input(\"\\n请输入问题(输入'exit'退出): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    print(\"\\n正在生成回答...\\n\")\n",
    "    response = query_engine.query(query)\n",
    "    response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex的生态系统正在快速发展，除了本文介绍的基础功能外，它还支持更多高级特性，如代理系统、知识图谱、查询引擎评估等。随着你的深入学习，这些功能将帮助你构建更强大、更智能的应用程序。\n",
    "希望本文能帮助你快速上手LlamaIndex，开始构建自己的RAG应用！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda12_0_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
